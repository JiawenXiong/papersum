+++
date = '2025-12-14T19:04:05+01:00'
draft = false
title = '记住我，优化我：一种用于经验驱动智能体演化的动态过程记忆框架'

+++

**英文标题：** Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution 

**作者：** Zouying Cao, Jiaji Deng, Li Yu, Weikang Zhou, Zhaoyang Liu, Bolin Ding, Hai Zhao

**单位：** 上海交通大学、通义实验室（阿里巴巴集团）

**发布时间：** 2025年12月11日

---

### 摘要

这篇论文提出了一种通用的动态过程记忆框架 ReMe，通过多维度提取经验、情境自适应重用和基于效用的持续优化，解决现有记忆系统的静态积累问题。实验证明，该框架能提升智能体的长期学习效果，甚至使较小模型超越更大基线，展示了记忆驱动智能体演化的潜力。

### 研究动机

现有大型语言模型（LLM）智能体通常使用“过程性记忆”来缓存历史经验，但这种记忆：

    1. **被动累积**，没有持续优化机制
    2. 难以提取高质量、可泛化的经验教训
    3. 不能随着智能体不断执行任务而动态进化

因此，研究目标是打破现有记忆方法的静态存储范式，提出一种 **动态演化记忆框架**，使得智能体能在连续交互中不断提炼经验、重用知识并调整记忆结构，从而提升长期学习能力。

### 主要方法与核心思想

#### 核心思想

提出一个名为 **ReMe（Remember Me, Refine Me）** 的框架，其目标是实现：

    1. 从经验数据中 **提取可重用、高质量知识**
    2. 根据当前情境 **动态检索和适配历史知识**
    3. 自动筛除无用记忆，实现 **记忆优化与修剪**

ReMe 不再是静态日志，而是一个 **闭环动态记忆系统**，能够推动智能体自我演化。

#### ReMe 框架的关键组成与流程

ReMe 框架包含 **三个互补联动的阶段**：

**1. 经验提取（Experience Acquisition）**

将智能体的执行轨迹（成功 + 失败）：

    1. 识别“成功模式”
    2. 分析失败根因
    3. 比较成功和失败案例
    4. 自动生成结构化经验（如：什么情境下怎么做、更好避免什么操作）

→ 构建高质量“经验粒度”存储单元。

**2. 经验重用（Experience Reuse）**

对于新的任务：

    1. 使用向量检索召回与当前任务最相关的历史经验
    2. 可选进行 **重排序与重新编写**（adaptive rewriting），让经验更契合当前上下文
    3. 将这些经验作为 in-context 提示帮助模型推理决策

强化智能体的动态适应能力，而非简单回放历史。

**3. 记忆优化（Experience Refinement）**

为避免无效经验积累：

    1. 对新成功轨迹执行选择性加入
    2. 对频繁检索但对性能提升极低的经验进行删除
    3. 引入失败反思机制辅助生成有用经验

→ 保持记忆库的 **紧凑性与长期适用性**。

### 实验及主要结果

ReMe 在两个基准 **BFCL-V3** 和 **AppWorld** 上进行了评估，并得出：

    1. 在引入 ReMe 记忆后，多项任务指标显著提升
    2. 小模型（如 Qwen3-8B + ReMe）表现超过大模型基线（如 Qwen3-14B 无记忆）
    3. 显现出 **记忆规模效应**：优质记忆能在一定程度替代模型参数规模提升模型性能

这说明 ReMe 有助于提高智能体的 **终身学习能力与资源效率**。

### 后续研究方向

可能的后续方向包括：

    1. **更细粒度经验结构化**（如微操作级别经验）
    2. **跨任务迁移记忆** 的泛化能力提升
    3. 与强化学习、真实交互环境集成
    4. 在长期对话、自主规划等领域实用化落地

### 论文来源

[2512.10696] Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution
https://arxiv.org/abs/2512.10696



